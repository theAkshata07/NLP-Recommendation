{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Matrix_ Factorization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFoT5xv5Ry52"
      },
      "outputs": [],
      "source": [
        "#low rank matrix factorization=to factorize the matrix into a product of two matrices with low dimensions.\n",
        "import numpy\n",
        "\n",
        "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
        "    ''' \n",
        "    R: rating matrix\n",
        "    P: |U| * K (User features matrix)\n",
        "    Q: |D| * K (Item features matrix)\n",
        "    K: latent features\n",
        "    steps: iterations\n",
        "    alpha: learning rate\n",
        "    beta: regularization parameter'''\n",
        "    Q = Q.T\n",
        "\n",
        "    for step in range(steps):\n",
        "        for i in range(len(R)):\n",
        "            for j in range(len(R[i])):\n",
        "                if R[i][j] > 0:\n",
        "                    # calculate error\n",
        "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
        "\n",
        "                    for k in range(K):\n",
        "                        # calculate gradient with a and beta parameter\n",
        "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
        "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
        "\n",
        "        eR = numpy.dot(P,Q)\n",
        "\n",
        "        e = 0\n",
        "\n",
        "        for i in range(len(R)):\n",
        "\n",
        "            for j in range(len(R[i])):\n",
        "\n",
        "                if R[i][j] > 0:\n",
        "\n",
        "                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2) #error square\n",
        "\n",
        "                    for k in range(K):\n",
        "\n",
        "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
        "        # 0.001: local minimum\n",
        "        if e < 0.001:\n",
        "\n",
        "            break\n",
        "\n",
        "    return P, Q.T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R = [\n",
        "\n",
        "     [5,3,0,1],\n",
        "\n",
        "     [4,0,0,1],\n",
        "\n",
        "     [1,1,0,5],\n",
        "\n",
        "     [1,0,0,4],\n",
        "\n",
        "     [0,1,5,4],\n",
        "    \n",
        "     [2,1,3,0],\n",
        "\n",
        "    ]\n",
        "\n",
        "R = numpy.array(R)\n",
        "# N: num of User\n",
        "N = len(R)\n",
        "# M: num of Movie\n",
        "M = len(R[0])\n",
        "# Num of Features\n",
        "K = 3\n",
        "\n",
        "\n",
        "P = numpy.random.rand(N,K)\n",
        "Q = numpy.random.rand(M,K)\n",
        "\n",
        " \n",
        "\n",
        "nP, nQ = matrix_factorization(R, P, Q, K)\n",
        "\n",
        "nR = numpy.dot(nP, nQ.T)"
      ],
      "metadata": {
        "id": "Xp32-BD9SCh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nP)\n",
        "print(nQ)\n",
        "print(nR)\n",
        "print(R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhG652V7SCj9",
        "outputId": "90248db1-4cca-4beb-ee48-d4fbe247fd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.27715337  1.06335302  2.1423708 ]\n",
            " [-0.15399908  1.23185136  1.43528581]\n",
            " [ 1.85575366  0.31403348  0.62191414]\n",
            " [ 1.46597035  0.40948127  0.45095105]\n",
            " [ 1.19264042  1.37296332  0.87024294]\n",
            " [ 0.80737855  0.57035741  0.77672933]]\n",
            "[[-0.19211354  1.18937812  1.72660605]\n",
            " [-0.00397845  0.00478617  1.35057043]\n",
            " [ 1.04080891  1.58177368  1.71906226]\n",
            " [ 2.41364183  0.48286775  0.53965333]]\n",
            "[[5.01700412 2.89961469 5.07638891 1.00064745]\n",
            " [3.97289551 1.9449631  4.2555721  0.99767944]\n",
            " [1.09078987 0.83405884 3.49732396 4.96637933]\n",
            " [0.98401012 0.6051687  2.94871463 3.9794099 ]\n",
            " [2.90641688 1.17715075 4.90902981 4.01119602]\n",
            " [1.86436783 1.04854537 3.0777492  2.64329441]]\n",
            "[[5 3 0 1]\n",
            " [4 0 0 1]\n",
            " [1 1 0 5]\n",
            " [1 0 0 4]\n",
            " [0 1 5 4]\n",
            " [2 1 3 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python code snippet to conduct the gradient descent algorithm. We set a rating matrix with 4 movies given by 6 users. As you can see, \n",
        "# some users didn’t watch some movies before, so the rating is given as 0 in the rating."
      ],
      "metadata": {
        "id": "Pi7lxBViSt_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predicted matrix is generated below. As you can see, the predicted matrix has similar output with the true values, and the 0 ratings \n",
        "# are replaced with the prediction based on the similar users’ preferences on movies.\n",
        "# In the real-world, the rating matrix is very sparse since every user watches movies at different frequencies. \n",
        "# However, the error function RMSE is only calculated with the non-null rating. The missing entries in the rating matrix would be \n",
        "# replaced by the dot product of the factor matrices. Therefore, we know what to recommend to the users with the unseen movies based \n",
        "# on the prediction."
      ],
      "metadata": {
        "id": "ZjbbN-THSuBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Conclusion:\n",
        "# Matrix factorization is a collaborative filtering method to find the relationship between items’ and users’ entities. Latent features, \n",
        "# the association between users and movies matrices, are determined to find similarity and make a prediction based on both item and user entities\n",
        "# The matrix factorization of user and item matrices can be generated when the math cost function RMSE is minimized through matrix factorization. \n",
        "# Gradient descent is a method to minimize the cost function."
      ],
      "metadata": {
        "id": "AZSHho6EUP-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XGO9QKnzUTbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}